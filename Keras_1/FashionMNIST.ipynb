{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set the seed for random operations. \n",
    "# This let our experiments to be reproducible. \n",
    "tf.random.set_seed(1234)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.data.Dataset.range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataset of sequential numbers\n",
    "# --------------------------------------\n",
    "print(\"Dataset.range examples:\")\n",
    "print(\"-----------------------\")\n",
    "\n",
    "range_dataset = tf.data.Dataset.range(0, 20, 1)\n",
    "\n",
    "print(\"\\n1. Dataset\")\n",
    "for el in range_dataset:\n",
    "    print(el)\n",
    "\n",
    "# Divide in batches\n",
    "bs = 3\n",
    "range_dataset = tf.data.Dataset.range(0, 20, 1).batch(bs, drop_remainder=False)\n",
    "\n",
    "print(\"\\n2. Dataset + batch\")\n",
    "for el in range_dataset:\n",
    "    print(el)\n",
    "\n",
    "# Apply a transformation to each element\n",
    "def map_fn(x):\n",
    "    return x**2\n",
    "\n",
    "range_dataset = tf.data.Dataset.range(0, 20, 1).batch(bs, drop_remainder=False).map(map_fn)\n",
    "\n",
    "print(\"\\n3. Dataset + batch + map\")\n",
    "for el in range_dataset:\n",
    "    print(el)\n",
    "\n",
    "# Filter dataset based on a condition\n",
    "def filter_fn(x):\n",
    "    return tf.equal(tf.math.mod(x, 2), 0)\n",
    "\n",
    "range_dataset = tf.data.Dataset.range(0, 20, 1).filter(filter_fn)\n",
    "\n",
    "print(\"\\n4. Dataset + filter\")\n",
    "for el in range_dataset:\n",
    "    print(el)\n",
    "\n",
    "# Random shuffling\n",
    "range_dataset = tf.data.Dataset.range(0, 20, 1).shuffle(\n",
    "    buffer_size=20, reshuffle_each_iteration=False, seed=1234).batch(bs)\n",
    "\n",
    "print(\"\\n5. Dataset + shuffle + batch\")\n",
    "for el in range_dataset:\n",
    "    print(el)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.data.Dataset.from_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset.from_tensors example:\n",
      "-----------------------------\n",
      "tf.Tensor([1 2 3 4 5 6 7 8 9], shape=(9,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Create Dataset as unique element\n",
    "# --------------------------------\n",
    "from_tensors_dataset = tf.data.Dataset.from_tensors([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "\n",
    "print(\"Dataset.from_tensors example:\")\n",
    "print(\"-----------------------------\")\n",
    "for el in from_tensors_dataset:\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.data.Dataset.from_tensor_slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset.from_tensor_slices example:\n",
      "-----------------------------\n",
      "(<tf.Tensor: shape=(2, 2), dtype=float64, numpy=\n",
      "array([[0.43527116, 0.58467553],\n",
      "       [0.99176275, 0.70880063]])>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      "(<tf.Tensor: shape=(2, 2), dtype=float64, numpy=\n",
      "array([[0.67072721, 0.50087774],\n",
      "       [0.61805936, 0.47731326]])>, <tf.Tensor: shape=(), dtype=int64, numpy=4>)\n",
      "(<tf.Tensor: shape=(2, 2), dtype=float64, numpy=\n",
      "array([[0.46599199, 0.90321091],\n",
      "       [0.92192004, 0.14551479]])>, <tf.Tensor: shape=(), dtype=int64, numpy=8>)\n",
      "(<tf.Tensor: shape=(2, 2), dtype=float64, numpy=\n",
      "array([[0.73472529, 0.14100383],\n",
      "       [0.45089969, 0.78578921]])>, <tf.Tensor: shape=(), dtype=int64, numpy=8>)\n",
      "(<tf.Tensor: shape=(2, 2), dtype=float64, numpy=\n",
      "array([[0.89323905, 0.63395722],\n",
      "       [0.38254406, 0.69997372]])>, <tf.Tensor: shape=(), dtype=int64, numpy=3>)\n",
      "(<tf.Tensor: shape=(2, 2), dtype=float64, numpy=\n",
      "array([[0.71768866, 0.13427392],\n",
      "       [0.20822038, 0.78586375]])>, <tf.Tensor: shape=(), dtype=int64, numpy=4>)\n",
      "(<tf.Tensor: shape=(2, 2), dtype=float64, numpy=\n",
      "array([[0.11914061, 0.01231735],\n",
      "       [0.10575954, 0.81583613]])>, <tf.Tensor: shape=(), dtype=int64, numpy=9>)\n",
      "(<tf.Tensor: shape=(2, 2), dtype=float64, numpy=\n",
      "array([[0.20741744, 0.80197634],\n",
      "       [0.49262552, 0.21940875]])>, <tf.Tensor: shape=(), dtype=int64, numpy=7>)\n",
      "(<tf.Tensor: shape=(2, 2), dtype=float64, numpy=\n",
      "array([[0.24589256, 0.74329289],\n",
      "       [0.7501204 , 0.57048427]])>, <tf.Tensor: shape=(), dtype=int64, numpy=8>)\n",
      "(<tf.Tensor: shape=(2, 2), dtype=float64, numpy=\n",
      "array([[0.87431727, 0.77598361],\n",
      "       [0.69881014, 0.20849446]])>, <tf.Tensor: shape=(), dtype=int64, numpy=3>)\n"
     ]
    }
   ],
   "source": [
    "# Create a Dataset of slices\n",
    "# --------------------------\n",
    "\n",
    "# All the elements must have the same size in first dimension (axis 0)\n",
    "from_tensor_slices_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (np.random.uniform(size=[10, 2, 2]), np.random.randint(10, size=[10])))\n",
    "\n",
    "print(\"Dataset.from_tensor_slices example:\")\n",
    "print(\"-----------------------------\")\n",
    "for el in from_tensor_slices_dataset:\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.data.Dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine multiple datasets\n",
    "# -------------------------\n",
    "x = tf.data.Dataset.from_tensor_slices(np.random.uniform(size=10))\n",
    "y = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "\n",
    "zipped = tf.data.Dataset.zip((x, y))\n",
    "\n",
    "print(\"Dataset.from_tensors example:\")\n",
    "print(\"-----------------------------\")\n",
    "for el in zipped:\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over range dataset\n",
    "# --------------------------\n",
    "\n",
    "# for a in b\n",
    "for el in zipped:\n",
    "    print(el)\n",
    "    \n",
    "print('\\n')\n",
    "    \n",
    "# for a in enumerate(b)\n",
    "for el_idx, el in enumerate(zipped):\n",
    "    print(el)\n",
    "    \n",
    "print('\\n')\n",
    "    \n",
    "# get iterator\n",
    "iterator = iter(zipped)\n",
    "print(next(iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Fashion MNIST - Multi-class classification\n",
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 1s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load built-in dataset\n",
    "# ---------------------\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "## From dataset info:\n",
    "'''\n",
    "Content\n",
    "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. The training and test data sets have 785 columns. The first column consists of the class labels (see above), and represents the article of clothing. The rest of the columns contain the pixel-values of the associated image.\n",
    "\n",
    "To locate a pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27. The pixel is located on row i and column j of a 28 x 28 matrix.\n",
    "For example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below.\n",
    "'''\n",
    "\n",
    "## [E]\n",
    "'''\n",
    "An image is (28,28) array, so in the cell below, we can see that the shape of x_train is (6000, 28, 28) since we have 6000 images\n",
    " - also, each position in the (28,28) array is a pixel, which is just a value from 1 to 255 that indicates darkness level.\n",
    "In y_train we have the type of the item in the picture, there are 10 types, the dataset indicates the correct one with an int\n",
    "We split the dataset into validation and training sets.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in training and validation sets\n",
    "# e.g., 50000 samples for training and 10000 samples for validation\n",
    "\n",
    "x_valid = x_train[50000:, ...] \n",
    "y_valid = y_train[50000:, ...] \n",
    "\n",
    "x_train = x_train[:50000, ...]\n",
    "y_train = y_train[:50000, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create Training Dataset object\n",
    "# ------------------------------\n",
    "## [E] combine x and y so for each image we have its respective type\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "\n",
    "# Shuffle\n",
    "train_dataset = train_dataset.shuffle(buffer_size=x_train.shape[0])\n",
    "\n",
    "# Normalize images\n",
    "## [E] Normalize the color (from 1 to 255) to 0 to 1\n",
    "def normalize_img(x_, y_):\n",
    "    return tf.cast(x_, tf.float32) / 255., y_\n",
    "\n",
    "train_dataset = train_dataset.map(normalize_img)\n",
    "\n",
    "# 1-hot encoding <- for categorical cross entropy\n",
    "## [E] instead of indicating the item type with an int, use a size 10 bool array with a 1 in the type position\n",
    "def to_categorical(x_, y_):\n",
    "    return x_, tf.one_hot(y_, depth=10)\n",
    "\n",
    "train_dataset = train_dataset.map(to_categorical)\n",
    "\n",
    "# Divide in batches\n",
    "bs = 32\n",
    "train_dataset = train_dataset.batch(bs)\n",
    "\n",
    "# Repeat\n",
    "# Without calling the repeat function the dataset \n",
    "# will be empty after consuming all the images\n",
    "train_dataset = train_dataset.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Validation Dataset  \n",
    "# -----------------------\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n",
    "\n",
    "# Normalize images\n",
    "valid_dataset = valid_dataset.map(normalize_img)\n",
    "\n",
    "# 1-hot encoding\n",
    "valid_dataset = valid_dataset.map(to_categorical)\n",
    "\n",
    "# Divide in batches\n",
    "valid_dataset = valid_dataset.batch(1)\n",
    "\n",
    "# Repeat\n",
    "valid_dataset = valid_dataset.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Test Dataset\n",
    "# -------------------\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "test_dataset = test_dataset.map(normalize_img)\n",
    "\n",
    "test_dataset = test_dataset.map(to_categorical)\n",
    "\n",
    "test_dataset = test_dataset.batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=128x128 at 0x7F38CC07A450>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(28, 28), dtype=float32, numpy=\n",
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.01176471, 0.        , 0.        , 0.07450981,\n",
       "        0.01568628, 0.        , 0.00392157, 0.        , 0.00784314,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.01176471, 0.        , 0.03137255, 0.5372549 , 0.4862745 ,\n",
       "        0.45490196, 0.3647059 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00784314,\n",
       "        0.        , 0.05098039, 0.68235296, 0.35686275, 0.        ,\n",
       "        0.        , 0.5137255 , 0.4509804 , 0.        , 0.01568628,\n",
       "        0.00392157, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.00392157, 0.        ,\n",
       "        0.        , 0.654902  , 0.39607844, 0.        , 0.02352941,\n",
       "        0.        , 0.        , 0.69411767, 0.21568628, 0.        ,\n",
       "        0.01568628, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.02352941, 0.        ,\n",
       "        0.34117648, 0.6156863 , 0.        , 0.03137255, 0.        ,\n",
       "        0.01176471, 0.        , 0.3019608 , 0.59607846, 0.        ,\n",
       "        0.02352941, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.5882353 , 0.11764706, 0.        , 0.01176471, 0.        ,\n",
       "        0.00784314, 0.        , 0.02745098, 0.6784314 , 0.10588235,\n",
       "        0.        , 0.00784314, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.02352941, 0.        , 0.22745098,\n",
       "        0.40784314, 0.        , 0.02352941, 0.        , 0.        ,\n",
       "        0.        , 0.01960784, 0.        , 0.47058824, 0.34901962,\n",
       "        0.        , 0.02745098, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.46666667,\n",
       "        0.07058824, 0.        , 0.01176471, 0.        , 0.        ,\n",
       "        0.        , 0.01176471, 0.        , 0.14509805, 0.47843137,\n",
       "        0.        , 0.00784314, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.01568628, 0.        , 0.25882354, 0.43137255,\n",
       "        0.        , 0.01568628, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00392157, 0.        , 0.44313726,\n",
       "        0.14901961, 0.        , 0.01568628, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.57254905, 0.13725491,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.01176471, 0.        , 0.2627451 ,\n",
       "        0.5137255 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.00392157,\n",
       "        0.00392157, 0.        , 0.03921569, 0.6509804 , 0.        ,\n",
       "        0.        , 0.01176471, 0.01568628, 0.00784314, 0.00784314,\n",
       "        0.00784314, 0.00392157, 0.02352941, 0.        , 0.04705882,\n",
       "        0.61960787, 0.        , 0.        , 0.        , 0.00392157,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.30588236, 0.77254903, 0.14509805,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.3529412 ,\n",
       "        0.7529412 , 0.19215687, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.07058824,\n",
       "        0.5647059 , 0.7137255 , 0.6901961 , 0.7137255 , 0.77254903,\n",
       "        0.5764706 , 0.21960784, 0.00392157, 0.        , 0.        ,\n",
       "        0.        , 0.06666667, 0.3019608 , 0.6431373 , 0.7254902 ,\n",
       "        0.7058824 , 0.7137255 , 0.74509805, 0.5176471 , 0.16862746,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.5686275 ,\n",
       "        0.7882353 , 0.7411765 , 0.7058824 , 0.7372549 , 0.6784314 ,\n",
       "        0.7607843 , 0.80784315, 0.7647059 , 0.7137255 , 0.72156864,\n",
       "        0.7372549 , 0.78431374, 0.80784315, 0.7372549 , 0.6862745 ,\n",
       "        0.70980394, 0.6862745 , 0.6666667 , 0.70980394, 0.627451  ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.5058824 ,\n",
       "        0.7254902 , 0.6392157 , 0.7294118 , 0.72156864, 0.6784314 ,\n",
       "        0.60784316, 0.6156863 , 0.6392157 , 0.6745098 , 0.70980394,\n",
       "        0.70980394, 0.654902  , 0.627451  , 0.6666667 , 0.6862745 ,\n",
       "        0.7254902 , 0.7137255 , 0.63529414, 0.6745098 , 0.4627451 ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.5254902 ,\n",
       "        0.6313726 , 0.61960787, 0.78039217, 0.6901961 , 0.6666667 ,\n",
       "        0.65882355, 0.6313726 , 0.64705884, 0.64705884, 0.68235296,\n",
       "        0.6784314 , 0.69411767, 0.6862745 , 0.6431373 , 0.6901961 ,\n",
       "        0.74509805, 0.7372549 , 0.58431375, 0.79607844, 0.58431375,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.61960787,\n",
       "        0.6117647 , 0.5686275 , 0.7529412 , 0.68235296, 0.6666667 ,\n",
       "        0.68235296, 0.59607846, 0.654902  , 0.627451  , 0.6392157 ,\n",
       "        0.654902  , 0.6901961 , 0.69803923, 0.6666667 , 0.75686276,\n",
       "        0.73333335, 0.7254902 , 0.64705884, 0.85882354, 0.57254905,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.57254905,\n",
       "        0.7058824 , 0.6666667 , 0.84705883, 0.85882354, 0.7921569 ,\n",
       "        0.654902  , 0.6156863 , 0.6862745 , 0.6666667 , 0.67058825,\n",
       "        0.65882355, 0.68235296, 0.70980394, 0.654902  , 0.6666667 ,\n",
       "        0.7607843 , 0.69803923, 0.60784316, 0.88235295, 0.5529412 ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.5372549 ,\n",
       "        0.62352943, 0.6039216 , 0.5411765 , 0.42352942, 0.74509805,\n",
       "        0.70980394, 0.69803923, 0.7019608 , 0.7058824 , 0.6901961 ,\n",
       "        0.6901961 , 0.6862745 , 0.7294118 , 0.69803923, 0.63529414,\n",
       "        0.74509805, 0.6745098 , 0.64705884, 0.8784314 , 0.5019608 ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.4862745 ,\n",
       "        0.6901961 , 0.6156863 , 0.54901963, 0.38039216, 0.7647059 ,\n",
       "        0.75686276, 0.7411765 , 0.7294118 , 0.72156864, 0.6862745 ,\n",
       "        0.72156864, 0.70980394, 0.7176471 , 0.70980394, 0.7058824 ,\n",
       "        0.6901961 , 0.6745098 , 0.6901961 , 0.8901961 , 0.4392157 ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.40784314,\n",
       "        0.7647059 , 0.5686275 , 0.89411765, 0.8509804 , 0.827451  ,\n",
       "        0.7607843 , 0.76862746, 0.75686276, 0.75686276, 0.7294118 ,\n",
       "        0.72156864, 0.7411765 , 0.7254902 , 0.7176471 , 0.7137255 ,\n",
       "        0.69803923, 0.67058825, 0.7137255 , 0.8392157 , 0.39607844,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.34509805,\n",
       "        0.8352941 , 0.5686275 , 0.76862746, 0.72156864, 0.7529412 ,\n",
       "        0.78039217, 0.7921569 , 0.78431374, 0.78431374, 0.77254903,\n",
       "        0.7647059 , 0.76862746, 0.7607843 , 0.7647059 , 0.7529412 ,\n",
       "        0.6784314 , 0.69803923, 0.74509805, 0.84313726, 0.3372549 ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.20392157,\n",
       "        0.87058824, 0.6392157 , 0.7137255 , 0.7294118 , 0.7882353 ,\n",
       "        0.7921569 , 0.8039216 , 0.8039216 , 0.8039216 , 0.7882353 ,\n",
       "        0.78431374, 0.7764706 , 0.78039217, 0.7764706 , 0.8       ,\n",
       "        0.7529412 , 0.69803923, 0.7529412 , 0.8352941 , 0.16078432,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.0627451 ,\n",
       "        0.8666667 , 0.63529414, 0.76862746, 0.79607844, 0.8235294 ,\n",
       "        0.8117647 , 0.8117647 , 0.8156863 , 0.81960785, 0.8117647 ,\n",
       "        0.80784315, 0.8       , 0.79607844, 0.79607844, 0.79607844,\n",
       "        0.8039216 , 0.7647059 , 0.7647059 , 0.8039216 , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.827451  , 0.7137255 , 0.80784315, 0.79607844, 0.7921569 ,\n",
       "        0.80784315, 0.8235294 , 0.81960785, 0.827451  , 0.84313726,\n",
       "        0.84313726, 0.84313726, 0.8352941 , 0.827451  , 0.8235294 ,\n",
       "        0.8235294 , 0.7882353 , 0.7921569 , 0.8666667 , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.54901963, 0.76862746, 0.7764706 , 0.7921569 , 0.80784315,\n",
       "        0.8117647 , 0.8117647 , 0.8117647 , 0.8117647 , 0.827451  ,\n",
       "        0.8352941 , 0.8352941 , 0.8352941 , 0.8235294 , 0.8156863 ,\n",
       "        0.80784315, 0.77254903, 0.7921569 , 0.5529412 , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.21960784, 0.85490197, 0.8666667 , 0.8862745 , 0.9764706 ,\n",
       "        0.9764706 , 0.98039216, 0.98039216, 0.98039216, 0.9843137 ,\n",
       "        0.9843137 , 0.9882353 , 1.        , 0.99215686, 0.8862745 ,\n",
       "        0.88235295, 0.8666667 , 0.88235295, 0.27450982, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.56078434, 0.6117647 , 0.56078434, 0.56078434,\n",
       "        0.5411765 , 0.5254902 , 0.5294118 , 0.5294118 , 0.49411765,\n",
       "        0.49019608, 0.49019608, 0.4392157 , 0.42745098, 0.43529412,\n",
       "        0.4392157 , 0.43137255, 0.4       , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]], dtype=float32)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that is everything is ok with the dataloader\n",
    "\n",
    "iterator = iter(train_dataset)\n",
    "sample, target = next(iterator)\n",
    "\n",
    "# Just for visualization purpouses\n",
    "sample_ = sample[0, ...]  # select first image in the batch\n",
    "\n",
    "from PIL import Image\n",
    "## [E] *255 since we have to un-normalize it (we wouldnt do this, its just to show the image)\n",
    "img = Image.fromarray(np.uint8(np.array(sample_)*255.))\n",
    "img = img.resize([128, 128])\n",
    "img\n",
    "\n",
    "sample_\n",
    "target[0]  # select corresponding target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fashion MNIST classification\n",
    "# ----------------------------\n",
    "\n",
    "# x: 28x28\n",
    "# y: 10 classes\n",
    "\n",
    "# Create Model\n",
    "# ------------\n",
    "# e.g. in: 28x28 -> h: 10 units -> out: 10 units (number of classes) \n",
    "\n",
    "# Define Input keras tensor\n",
    "x = tf.keras.Input(shape=[28, 28])\n",
    "\n",
    "# Define intermediate hidden layers and chain\n",
    "flatten = tf.keras.layers.Flatten()(x)  \n",
    "h = tf.keras.layers.Dense(units=10, activation=tf.keras.activations.sigmoid)(flatten)\n",
    "\n",
    "# Define output layer and chain\n",
    "\n",
    "# Define the last fully-connected layer, which is composed by 10 neurons (the number of classes). \n",
    "# Finally, the softmax activation function is applied for multiclass classification\n",
    "out = tf.keras.layers.Dense(units=10, activation=tf.keras.activations.softmax)(h) \n",
    "\n",
    "# Create Model instance defining inputs and outputs\n",
    "model = tf.keras.Model(inputs=x, outputs=out) # Note: you can have a model with multiple inputs and multiple outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                110       \n",
      "=================================================================\n",
      "Total params: 7,960\n",
      "Trainable params: 7,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_2/kernel:0' shape=(784, 10) dtype=float32, numpy=\n",
       " array([[ 0.03069539, -0.03156194,  0.03554446, ..., -0.00087159,\n",
       "         -0.03436801,  0.00915697],\n",
       "        [-0.00326168, -0.08637477,  0.0588085 , ...,  0.00483081,\n",
       "         -0.03063906, -0.06936873],\n",
       "        [-0.08279558,  0.05901644,  0.06045129, ...,  0.00170612,\n",
       "          0.05668575,  0.00926068],\n",
       "        ...,\n",
       "        [-0.02512205, -0.05552066,  0.01029567, ..., -0.03475806,\n",
       "         -0.05140539,  0.07158359],\n",
       "        [-0.04862122, -0.04219229, -0.02208634, ..., -0.00336875,\n",
       "         -0.02109279,  0.0018597 ],\n",
       "        [ 0.02661907, -0.01737248,  0.08323707, ..., -0.04416673,\n",
       "          0.0566181 , -0.03901453]], dtype=float32)>,\n",
       " <tf.Variable 'dense_2/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'dense_3/kernel:0' shape=(10, 10) dtype=float32, numpy=\n",
       " array([[-0.22314459,  0.18020284,  0.01730126, -0.3705793 , -0.24604946,\n",
       "         -0.46697396, -0.5175091 ,  0.5309746 ,  0.051916  ,  0.06130654],\n",
       "        [ 0.20811611,  0.13667864,  0.23374999, -0.30489117,  0.09954989,\n",
       "         -0.4819057 , -0.25458282,  0.07266605, -0.18388876,  0.3762713 ],\n",
       "        [-0.23230046,  0.4274004 , -0.18700418,  0.13199991,  0.00711519,\n",
       "          0.08032918,  0.21152014,  0.2979498 ,  0.38175505, -0.19068661],\n",
       "        [-0.18346709,  0.35563987,  0.392245  ,  0.16770405,  0.42513055,\n",
       "         -0.16906151, -0.50546235,  0.17452252,  0.05459225,  0.31085247],\n",
       "        [ 0.06772029, -0.4136715 , -0.06327239,  0.3831346 ,  0.4030574 ,\n",
       "          0.4573592 ,  0.33951205, -0.10501486,  0.18275422,  0.23944074],\n",
       "        [ 0.03549623,  0.01924866,  0.5309204 ,  0.3880521 ,  0.14003003,\n",
       "         -0.26325616,  0.33819342,  0.04438412,  0.08560401,  0.36554492],\n",
       "        [-0.53116894, -0.07626152, -0.2249851 ,  0.15800416,  0.20132959,\n",
       "          0.09648031,  0.34875906, -0.52098835, -0.3490337 ,  0.09489137],\n",
       "        [-0.40825802, -0.40638512,  0.0854606 ,  0.28571546,  0.00107396,\n",
       "         -0.19690883,  0.41307354,  0.33342618, -0.46568704, -0.08078256],\n",
       "        [-0.41725445, -0.4007281 , -0.11128122,  0.34646988,  0.34148628,\n",
       "          0.13383102,  0.489684  ,  0.4172334 , -0.13726065,  0.19606459],\n",
       "        [ 0.17388982, -0.49581102, -0.36021668,  0.21571565,  0.10798883,\n",
       "         -0.22900549,  0.37763685, -0.12425518, -0.37668738, -0.27821887]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_3/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize created model as a table\n",
    "\n",
    "# I can visualise the model I create and the weights initialisation\n",
    "model.summary()\n",
    "\n",
    "# Visualize initialized weights\n",
    "model.weights\n",
    "# As you can see it will show you the size of the output of your layers and the number of parameters (weights + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent formulation\n",
    "# ----------------------\n",
    "\n",
    "# Create model with sequential \n",
    "# (uncomment to run)\n",
    "# seq_model = tf.keras.Sequential()\n",
    "# seq_model.add(tf.keras.layers.Flatten(input_shape=(28, 28))) # or as a list\n",
    "# seq_model.add(tf.keras.layers.Dense(units=10, activation=tf.keras.activations.sigmoid))\n",
    "# seq_model.add(tf.keras.layers.Dense(units=10, activation=tf.keras.activations.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_model.summary()\n",
    "# seq_model.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization params\n",
    "# -------------------\n",
    "\n",
    "# Before training the network we have to 'compile' the model by defining the following hyperparameters\n",
    "\n",
    "# Loss\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# Learning rate\n",
    "lr = 1e-3\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "# -------------------\n",
    "\n",
    "# Validation metrics\n",
    "# ------------------\n",
    "\n",
    "# Define the metrics we want to compute during validation \n",
    "# (keras will automatically evaluate them also on the training set). \n",
    "# In this example we compute the accuracy, i.e., the frequency of correctly predicted classes\n",
    "\n",
    "metrics = ['accuracy']\n",
    "# ------------------\n",
    "\n",
    "# Finally, we call model.compile\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1558/1562 [============================>.] - ETA: 0s - loss: 0.3912 - accuracy: 0.8614WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'logs' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-599beedc4ad5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# how many batches per epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m           validation_steps=10000)  # number of batches in validation set\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Here we are! While the network is training keras provides useful information.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    870\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m               return_dict=True)\n\u001b[0m\u001b[1;32m    873\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1089\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1092\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'logs' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# Now we are ready to start training our network. \n",
    "# This is done by calling the 'fit' function. We need to set some parameters. The main ones are:\n",
    "\n",
    "# 1) Training set. \n",
    "# 'x' and 'y' params represent the input and targets for training, respectively. \n",
    "# If we have a dataloader which already provides <input, target> pairs, we have to set only the 'x' param.\n",
    "# If you have a very small dataset, it could be convenient instead to set 'x' and 'y' directly with your\n",
    "# numpy arrays containing all your training set (e.g., x = x_train, y = y_train in the example).\n",
    "\n",
    "# 2) Number of epochs (how many times we want to process all the dataset).\n",
    "\n",
    "# 3) Steps per epoch, i.e., (# training images) / (batch size)\n",
    "\n",
    "# 4) Validation set\n",
    "# 'validation_data' will be our dataloader for the validation set (also in this case we can give the numpy array directly).\n",
    "# 'validation_steps' is similar to the 'steps_per_epoch'. In the example it is 10000, since we have \n",
    "# chosen arbitrarily a batch size of 1 sample for the validation dataset. \n",
    "\n",
    "model.fit(x=train_dataset,  # you can give directly numpy arrays x_train\n",
    "          y=None,   \n",
    "          epochs=10, \n",
    "          steps_per_epoch=int(np.ceil(x_train.shape[0] / bs)),  # how many batches per epoch\n",
    "          validation_data=valid_dataset,  \n",
    "          validation_steps=10000)  # number of batches in validation set\n",
    "\n",
    "# Here we are! While the network is training keras provides useful information. \n",
    "# In particular, we can see the loss and the metrics computed on training batches, and, at the end of each epoch,\n",
    "# the same quantities are computed on the validation set (very useful to understand if we are overfitting..).\n",
    "# But we can also inspect these and other details in a smarter way..see you next lesson!:) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
