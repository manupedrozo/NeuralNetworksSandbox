{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FiLwi7HcT6Zm"
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJqHEF3eT6Zn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set the seed for random operations. \n",
    "# This let our experiments to be reproducible. \n",
    "SEED = 1234\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Get current working directory\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41UUNZOHWyfX"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pu6Q_E17XHJR"
   },
   "outputs": [],
   "source": [
    "!cat /content/drive/My\\ Drive/ita.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAIMIdbNT6Zo"
   },
   "source": [
    "# Neural Machine Translation \n",
    "## Italian/English\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BeszjH0iT6Zo"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBKw-g6cT6Zo"
   },
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "# ---------------\n",
    "\n",
    "MAX_NUM_SENTENCES = 40000\n",
    "MAX_NUM_WORDS = 20000\n",
    "\n",
    "ita_sentences = []\n",
    "eng_sentences = []\n",
    "eng_sentences_train = []\n",
    "\n",
    "# Simplify the dataset\n",
    "MAX_LEN = 3 # words\n",
    "\n",
    "# Read all lines in translation dataset\n",
    "count = 0\n",
    "for line in open(os.path.join('/content/drive/My Drive', 'ita.txt'), encoding='utf-8'):\n",
    "    \n",
    "    if count > MAX_NUM_SENTENCES:\n",
    "        break\n",
    "\n",
    "    if '\\t' not in line:\n",
    "        continue\n",
    "        \n",
    "    eng_sentence_, ita_sentence, _ = line.rstrip().split('\\t')\n",
    "\n",
    "    if (len(eng_sentence_.split(' ')) > MAX_LEN or\n",
    "          len(ita_sentence.split(' ')) > MAX_LEN):\n",
    "      continue\n",
    "    \n",
    "    eng_sentence = eng_sentence_ + ' <eos>'\n",
    "    eng_sentence_train = '<sos> ' + eng_sentence_\n",
    "    \n",
    "    ita_sentences.append(ita_sentence)\n",
    "    eng_sentences.append(eng_sentence)\n",
    "    eng_sentences_train.append(eng_sentence_train)\n",
    "\n",
    "    count += 1\n",
    "\n",
    "print('Number of sentences:', len(ita_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AHVnghuyXV0l"
   },
   "outputs": [],
   "source": [
    "max(len(sentence.split(' ')) for sentence in eng_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75sNY9KBT6Zo"
   },
   "source": [
    "# Tokenization\n",
    "## Converts words to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ADaD89lT6Zo"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Create Tokenizer to convert words to integers\n",
    "ita_tokenizer = Tokenizer(num_words= MAX_NUM_WORDS)\n",
    "ita_tokenizer.fit_on_texts(ita_sentences)\n",
    "ita_tokenized = ita_tokenizer.texts_to_sequences(ita_sentences)\n",
    "\n",
    "ita_wtoi = ita_tokenizer.word_index\n",
    "print('Total italian words:', len(ita_wtoi))\n",
    "\n",
    "max_ita_length = max(len(sentence) for sentence in ita_tokenized)\n",
    "print('Max italian sentence length:', max_ita_length)\n",
    "\n",
    "eng_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, filters='?!,.\"')\n",
    "eng_tokenizer.fit_on_texts(eng_sentences+eng_sentences_train)\n",
    "eng_tokenized = eng_tokenizer.texts_to_sequences(eng_sentences)\n",
    "eng_tokenized_train = eng_tokenizer.texts_to_sequences(eng_sentences_train)\n",
    "\n",
    "eng_wtoi = eng_tokenizer.word_index\n",
    "print('Total english words:', len(eng_wtoi))\n",
    "\n",
    "max_eng_length = max(len(sentence) for sentence in eng_tokenized)\n",
    "print('Max english sentence length:', max_eng_length)\n",
    "\n",
    "num_eng_words = len(eng_wtoi) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMHmTPzdT6Zo"
   },
   "source": [
    "# Padding sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SsAMc718T6Zp"
   },
   "outputs": [],
   "source": [
    "# Pad to max italian sentence length\n",
    "ita_encoder_inputs = pad_sequences(ita_tokenized, maxlen=max_ita_length)\n",
    "\n",
    "print(\"Italian encoder inputs shape:\", ita_encoder_inputs.shape)\n",
    "\n",
    "# Pad to max italian sentence length\n",
    "eng_decoder_inputs = pad_sequences(eng_tokenized_train, maxlen=max_eng_length, padding='post')\n",
    "\n",
    "print(\"English decoder inputs shape:\", eng_decoder_inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YIJCEoWST6Zp"
   },
   "outputs": [],
   "source": [
    "# Pad to max english sentence length\n",
    "eng_outputs = pad_sequences(eng_tokenized, maxlen=max_eng_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G58-ZmdtT6Zp"
   },
   "outputs": [],
   "source": [
    "ita_encoder_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fyx28iuHT6Zp"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8IC7MPwGT6Zp"
   },
   "outputs": [],
   "source": [
    "# Build Encoder-Decoder Model\n",
    "# ---------------------------\n",
    "\n",
    "EMBEDDING_SIZE = 32\n",
    "\n",
    "# ENCODER\n",
    "# -------\n",
    "\n",
    "encoder_input = tf.keras.Input(shape=[max_ita_length])\n",
    "encoder_embedding_layer = tf.keras.layers.Embedding(len(ita_wtoi)+1, EMBEDDING_SIZE, input_length=max_ita_length, mask_zero=True)\n",
    "encoder_embedding_out = encoder_embedding_layer(encoder_input)\n",
    "encoder = tf.keras.layers.LSTM(units=128, return_state=True)\n",
    "\n",
    "encoder_output, h, c = encoder(encoder_embedding_out)\n",
    "encoder_states = [h, c]\n",
    "\n",
    "# DECODER\n",
    "# -------\n",
    "\n",
    "decoder_input = tf.keras.Input(shape=[max_eng_length])\n",
    "decoder_embedding_layer = tf.keras.layers.Embedding(len(eng_wtoi)+1, EMBEDDING_SIZE)\n",
    "decoder_embedding_out = decoder_embedding_layer(decoder_input)\n",
    "decoder_lstm = tf.keras.layers.LSTM(units=128, return_sequences=True, return_state=True)\n",
    "\n",
    "# Initialize decoder state with final encoder state (initial_state=encoder_states)\n",
    "decoder_lstm_out, _, _ = decoder_lstm(decoder_embedding_out, initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = tf.keras.layers.Dense(len(eng_wtoi)+1, activation='softmax')\n",
    "decoder = decoder_dense(decoder_lstm_out)\n",
    "\n",
    "# MODEL\n",
    "model = tf.keras.Model([encoder_input, decoder_input], decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CBGKVJNYT6Zp"
   },
   "outputs": [],
   "source": [
    "model.summary()\n",
    "model.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4Lw7k0pT6Zp"
   },
   "source": [
    "# Prepare model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_ihm4kvT6Zp"
   },
   "outputs": [],
   "source": [
    "# Optimization params\n",
    "# -------------------\n",
    "\n",
    "# Loss\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# learning rate\n",
    "lr = 1e-3\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "# -------------------\n",
    "\n",
    "# Validation metrics\n",
    "# ------------------\n",
    "\n",
    "metrics = ['accuracy']\n",
    "# ------------------\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlbGCZQUT6Zp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "exps_dir = os.path.join('/content/drive/My Drive/KerasRNN', 'translation_experiments')\n",
    "if not os.path.exists(exps_dir):\n",
    "    os.makedirs(exps_dir)\n",
    "\n",
    "now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "\n",
    "exp_name = 'exp'\n",
    "\n",
    "exp_dir = os.path.join(exps_dir, exp_name + '_' + str(now))\n",
    "if not os.path.exists(exp_dir):\n",
    "    os.makedirs(exp_dir)\n",
    "    \n",
    "callbacks = []\n",
    "\n",
    "# Model checkpoint\n",
    "# ----------------\n",
    "ckpt_dir = os.path.join(exp_dir, 'ckpts')\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "\n",
    "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \n",
    "                                                   save_weights_only=True)  # False to save the model directly\n",
    "callbacks.append(ckpt_callback)\n",
    "\n",
    "# ----------------\n",
    "\n",
    "# Visualize Learning on Tensorboard\n",
    "# ---------------------------------\n",
    "tb_dir = os.path.join(exp_dir, 'tb_logs')\n",
    "if not os.path.exists(tb_dir):\n",
    "    os.makedirs(tb_dir)\n",
    "    \n",
    "# By default shows losses and metrics for both training and validation\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n",
    "                                             profile_batch=0,\n",
    "                                             histogram_freq=1)  # if 1 shows weights histograms\n",
    "callbacks.append(tb_callback)\n",
    "\n",
    "# Early Stopping\n",
    "# --------------\n",
    "early_stop = False\n",
    "if early_stop:\n",
    "    es_callback = tf.keras.callback.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    callbacks.append(es_callback)\n",
    "\n",
    "# ---------------------------------\n",
    "\n",
    "model.fit([ita_encoder_inputs, eng_decoder_inputs],\n",
    "          eng_outputs,\n",
    "          epochs=100,\n",
    "          batch_size=128, \n",
    "          validation_split=0.2, \n",
    "          callbacks=callbacks)\n",
    "\n",
    "# How to visualize Tensorboard\n",
    "\n",
    "# 1. tensorboard --logdir EXPERIMENTS_DIR --port PORT     <- from terminal\n",
    "# 2. localhost:PORT   <- in your browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNP5ZjEgT6Zp"
   },
   "source": [
    "# Translation (inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0PmgVc7yT6Zp"
   },
   "outputs": [],
   "source": [
    "# Uncomment this to load model\n",
    "model.load_weights(os.path.join('/content/drive/My Drive/KerasRNN', 'translation_experiments/exp_Dec04_02-03-59', 'ckpts/cp_58.ckpt'))  # use this if you want to restore saved model\n",
    "\n",
    "# Modify the model such that the decoder takes prdictions as inputs (no teacher forcing)\n",
    "\n",
    "# ENCODER (remains the same)\n",
    "# -------\n",
    "encoder_model = tf.keras.Model(encoder_input, encoder_states)\n",
    "\n",
    "# DECODER (modified)\n",
    "# ------------------\n",
    "decoder_state_input_h = tf.keras.Input(shape=[128])\n",
    "decoder_state_input_c = tf.keras.Input(shape=[128])\n",
    "decoder_state_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_input_single = tf.keras.Input(shape=[1])\n",
    "decoder_input_single_embedding = decoder_embedding_layer(decoder_input_single)\n",
    "decoder_outputs, h, c = decoder_lstm(decoder_input_single_embedding, initial_state=decoder_state_inputs)\n",
    "\n",
    "decoder_states = [h, c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = tf.keras.Model([decoder_input_single] + decoder_state_inputs,\n",
    "                               [decoder_outputs]+decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gm7nVJqnT6Zp"
   },
   "outputs": [],
   "source": [
    "# Translation utils\n",
    "ita_itow = {v:k for k, v in ita_wtoi.items()}\n",
    "eng_itow = {v:k for k, v in eng_wtoi.items()}\n",
    "\n",
    "def translate(input_sentence):\n",
    "    \n",
    "    # Prepare input sentence\n",
    "    input_tokenized = ita_tokenizer.texts_to_sequences([input_sentence])\n",
    "    input_tokenized = pad_sequences(input_tokenized, maxlen=max_ita_length)\n",
    "    \n",
    "    # Get encoder state\n",
    "    states_value = encoder_model.predict(input_tokenized)\n",
    "    \n",
    "    # Set first input '<sos>'\n",
    "    curr_input = np.zeros([1, 1])  # bs x seq_length (1 x 1 at the beginning)\n",
    "    curr_input[0, 0] = eng_wtoi['<sos>']\n",
    "    eos = eng_wtoi['<eos>']\n",
    "    \n",
    "    output_sentence = []\n",
    "    \n",
    "    # Cycle until max_eng_length or until the '<eos>' is predicted\n",
    "    for _ in range(max_eng_length):\n",
    "        preds, h_, c_ = decoder_model.predict([curr_input]+states_value)\n",
    "        word_id = np.argmax(preds[0, 0, :])\n",
    "        \n",
    "        if eos == word_id:\n",
    "            break\n",
    "        \n",
    "        word = ''\n",
    "        \n",
    "        if word_id > 0:\n",
    "            word = eng_itow[word_id]\n",
    "            output_sentence.append(word)\n",
    "            \n",
    "        # Update next input with the predicted one\n",
    "        curr_input[0, 0] = word_id\n",
    "        # Update state\n",
    "        states_value = [h_, c_]\n",
    "    \n",
    "    return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3dElxDoT6Zq"
   },
   "outputs": [],
   "source": [
    "print(translate(\"Ciao\"))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ita-Eng_Translation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
