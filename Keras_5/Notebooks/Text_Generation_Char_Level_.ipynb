{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVMaCBlrpHX-"
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Shzj3oKLpHX-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set the seed for random operations. \n",
    "# This let our experiments to be reproducible. \n",
    "SEED = 1234\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Get current working directory\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6QEuvmfzpVyL"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zurvj2CNq2uf"
   },
   "outputs": [],
   "source": [
    "!cat /content/drive/My\\ Drive/Cantico_di_Natale.txt  # Try to change the text with the one you prefer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRLxRzkKpHX_"
   },
   "source": [
    "# Text generation - Next character prediction\n",
    "## Charles Dickens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pn62ThPXJmnA"
   },
   "outputs": [],
   "source": [
    "# Given the text we will:\n",
    "# 1: divide the text into subsequences of N chars\n",
    "# 2: build (input, target) pairs, where input is a sequence and target is the \n",
    "#    same sequence shifted of 1 char\n",
    "# 3: train a recurrent neural network to predict the next char after each input char\n",
    "# 4: use the learned model to generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NXAYqXUpHX_"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JzQT-_-KpHX_"
   },
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "# ---------------\n",
    "\n",
    "# Read full text\n",
    "with open(os.path.join('/content/drive/My Drive/Cantico_di_Natale.txt'), 'r') as f:\n",
    "    full_text = f.read()\n",
    "\n",
    "full_text_length = len(full_text)\n",
    "print('Full text length:', full_text_length)\n",
    "\n",
    "# Create vocabulary\n",
    "vocabulary = sorted(list(set(full_text)))\n",
    "\n",
    "print('Number of unique characters:', len(vocabulary))\n",
    "print(vocabulary)\n",
    "\n",
    "# Dictionaries for char-to-int/int-to-char conversion\n",
    "ctoi = {c:i for i, c in enumerate(vocabulary)}\n",
    "itoc = {i:c for i, c in enumerate(vocabulary)}\n",
    "\n",
    "# Create input-target pairs\n",
    "# e.g., given an input sequence \n",
    "# 'Hell' predict the next characters 'ello'\n",
    "# Thus,\n",
    "# extract from the full text sequences of length seq_length as x and \n",
    "# the corresponding seq_length+1 character as target\n",
    "\n",
    "# Define number of characters to be considered for the prediction\n",
    "seq_length = 100\n",
    "\n",
    "X = [] # will contain all the sequences \n",
    "Y = [] # will contain for each sequence in X the next characters\n",
    "X_enc = []\n",
    "Y_enc = []\n",
    "# Cycle over the full text\n",
    "step = 1 \n",
    "for i in range(0, full_text_length - (seq_length), step):\n",
    "    sequence = full_text[i:i+seq_length]\n",
    "    target = full_text[i+1:i+seq_length+1]\n",
    "    X.append(sequence)\n",
    "    Y.append(target)\n",
    "    X_enc.append([ctoi[c] for c in sequence])\n",
    "    Y_enc.append([ctoi[c] for c in target])\n",
    "    \n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "X_enc = np.array(X_enc)\n",
    "Y_enc = np.array(Y_enc)\n",
    "    \n",
    "print('Number of sequences in the dataset:', len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lFFEjCPWU6w"
   },
   "outputs": [],
   "source": [
    "print(\"Input Sequence: {}\".format(X[0]))\n",
    "print(\"Target Sequence: {}\".format(Y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OiC9fTUipHX_"
   },
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "# -------------------\n",
    "\n",
    "# Batch size\n",
    "bs = 256\n",
    "\n",
    "# Encode characters. Many ways, for example one-hot encoding.\n",
    "def char_encode(x_, y_):\n",
    "    return tf.one_hot(x_, len(vocabulary)), tf.one_hot(y_, len(vocabulary))\n",
    "\n",
    "# Prepare input x to match recurrent layer input shape \n",
    "# -> (bs, seq_length, input_size)\n",
    "\n",
    "# Training\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_enc, Y_enc))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=X_enc.shape[0])\n",
    "train_dataset = train_dataset.map(char_encode)\n",
    "train_dataset = train_dataset.batch(bs)\n",
    "train_dataset = train_dataset.repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LpFlBk8CpHX_"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ykG52pUopHX_"
   },
   "outputs": [],
   "source": [
    "# Build Recurrent Neural Network\n",
    "# ------------------------------\n",
    "\n",
    "# We build two models, one for training and one for inference. The two models \n",
    "# SHARE THE WEIGHTS, thus the inference model will use the learned weights after training\n",
    "\n",
    "# Training and inference model differ only for the initialization of the state \n",
    "# in the lstm. In the inference model the state of the lstm is obtained through \n",
    "# input layers. In this way, we can provide the prediction at time t-1 as input \n",
    "# at time t for the generation of text at inference time.\n",
    "\n",
    "# Model architecture (2 stacked lstm layers): Input -> LSTM-1 -> LSTM-2 -> Dense \n",
    "\n",
    "# Hidden size (state)\n",
    "h_size = 128\n",
    "\n",
    "# Model\n",
    "input_x = tf.keras.Input(shape=(None, len(vocabulary)))\n",
    "\n",
    "lstm1 = tf.keras.layers.LSTM(\n",
    "    units=h_size, batch_input_shape=[None, None, len(vocabulary)], \n",
    "    return_sequences=True, return_state=True, stateful=False)\n",
    "lstm2 = tf.keras.layers.LSTM(\n",
    "    units=h_size, return_sequences=True, \n",
    "    return_state=True, stateful=False)\n",
    "dense = tf.keras.layers.Dense(units=len(vocabulary), activation='softmax')\n",
    "\n",
    "x, _, _ = lstm1(input_x)\n",
    "x, _, _ = lstm2(x)\n",
    "out = dense(x)\n",
    "\n",
    "train_model = tf.keras.Model(\n",
    "    inputs=input_x, outputs=out)\n",
    "\n",
    "# Inference Model\n",
    "h1_in = tf.keras.Input(shape=[h_size])\n",
    "c1_in = tf.keras.Input(shape=[h_size])\n",
    "h2_in = tf.keras.Input(shape=[h_size])\n",
    "c2_in = tf.keras.Input(shape=[h_size])\n",
    "\n",
    "x, h1, c1 = lstm1(input_x, initial_state=[h1_in, c1_in])\n",
    "x, h2, c2 = lstm2(x, initial_state=[h2_in, c2_in])\n",
    "out = dense(x)\n",
    "\n",
    "inference_model = tf.keras.Model(\n",
    "    inputs=[input_x, h1_in, c1_in, h2_in, c2_in], \n",
    "    outputs=[out, h1, c1, h2, c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebWNxVfzpHX_"
   },
   "outputs": [],
   "source": [
    "train_model.summary()\n",
    "\n",
    "inference_model.summary()\n",
    "#model.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJK77XwmpHX_"
   },
   "source": [
    "# Prepare model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VNTwzUzVpHX_"
   },
   "outputs": [],
   "source": [
    "# Optimization params\n",
    "# -------------------\n",
    "\n",
    "# Loss\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# learning rate\n",
    "lr = 1e-3\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "# -------------------\n",
    "\n",
    "# Validation metrics\n",
    "# ------------------\n",
    "\n",
    "metrics = ['accuracy']\n",
    "# ------------------\n",
    "\n",
    "# Compile Model\n",
    "train_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eg8s_AQRpHX_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "exps_dir = os.path.join('/content/drive/My Drive/KerasRNN', 'text_gen_experiments')\n",
    "if not os.path.exists(exps_dir):\n",
    "    os.makedirs(exps_dir)\n",
    "\n",
    "now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "\n",
    "exp_name = 'exp'\n",
    "\n",
    "exp_dir = os.path.join(exps_dir, exp_name + '_' + str(now))\n",
    "if not os.path.exists(exp_dir):\n",
    "    os.makedirs(exp_dir)\n",
    "    \n",
    "callbacks = []\n",
    "\n",
    "# Model checkpoint\n",
    "# ----------------\n",
    "ckpt_dir = os.path.join(exp_dir, 'ckpts')\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "\n",
    "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \n",
    "                                                   save_weights_only=True)  # False to save the model directly\n",
    "callbacks.append(ckpt_callback)\n",
    "\n",
    "# ----------------\n",
    "\n",
    "# Visualize Learning on Tensorboard\n",
    "# ---------------------------------\n",
    "tb_dir = os.path.join(exp_dir, 'tb_logs')\n",
    "if not os.path.exists(tb_dir):\n",
    "    os.makedirs(tb_dir)\n",
    "    \n",
    "# By default shows losses and metrics for both training and validation\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n",
    "                                             profile_batch=0,\n",
    "                                             histogram_freq=1)  # if 1 shows weights histograms\n",
    "callbacks.append(tb_callback)\n",
    "\n",
    "# Early Stopping\n",
    "# --------------\n",
    "early_stop = False\n",
    "if early_stop:\n",
    "    es_callback = tf.keras.callback.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    callbacks.append(es_callback)\n",
    "\n",
    "# ---------------------------------\n",
    "\n",
    "# How to visualize Tensorboard\n",
    "\n",
    "# 1. tensorboard --logdir EXPERIMENTS_DIR --port PORT     <- from terminal\n",
    "# 2. localhost:PORT   <- in your browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WzuSH__3dnrY"
   },
   "outputs": [],
   "source": [
    "# It is difficult to have a validation metric to evaluate quantitatively the quality\n",
    "# of a generation, since we are generating new text. Thus, for example, we can \n",
    "# evaluate qualitatively the generation performance by generating some text during validation.\n",
    "# We can do it through a custom callback which, at the beginning of each epoch (on_epoch_begin),\n",
    "# will take a list of test sequences and use the trained model to generate some text.\n",
    "\n",
    "class TextGenerationCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "  def __init__(self, inference_model, start_sequences, \n",
    "               generation_length, temperature):\n",
    "    super(TextGenerationCallback, self).__init__()\n",
    "\n",
    "    self.inference_model = inference_model\n",
    "    self.start_sequences = start_sequences\n",
    "    self.generation_length = generation_length\n",
    "    self.temperature = temperature\n",
    "\n",
    "  def sample(self, pred, temperature=1.0):\n",
    "    # Helper function to sample an index from a probability array\n",
    "    # Temperature is a parameter that allows to scale the output of the network, \n",
    "    # thus changing the level of 'exploration' in the output distribution. Try\n",
    "    # yourself to play with this parameter and to see the difference in the generation.\n",
    "    pred = np.asarray(pred).astype('float64')\n",
    "    pred = np.log(pred) / temperature\n",
    "    exp_pred = np.exp(pred)\n",
    "    pred = exp_pred / np.sum(exp_pred)\n",
    "    probas = np.random.multinomial(1, pred, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "  def generate_text(self, start_sequence, temperature):\n",
    "    encoded_input = [ctoi[c] for c in start_sequence]\n",
    "    encoded_input = tf.one_hot(encoded_input, len(vocabulary))\n",
    "    encoded_input = tf.expand_dims(encoded_input, 0)\n",
    "\n",
    "    generated_sequence = start_sequence\n",
    "\n",
    "    in_states = [tf.zeros([1, h_size]) for _ in range(4)]\n",
    "\n",
    "    for i in range(self.generation_length):\n",
    "      output = self.inference_model.predict([encoded_input] + in_states)\n",
    "\n",
    "      in_states = output[1:]\n",
    "      pred = output[0][0, -1]\n",
    "      # To get the final prediction we should use the argmax as usual but if we \n",
    "      # do so, we will take always the most probable char, so we could incur into a generation loop.\n",
    "      # Thus, instead of taking the max probability, we sample from the output distribution, \n",
    "      # giving a chance to explore also other chars during the generation.\n",
    "      # We do it with an helper function, that is the 'sample' one. \n",
    "      pred = self.sample(pred, temperature)\n",
    "      pred_char = itoc[pred]\n",
    "      \n",
    "      generated_sequence += pred_char\n",
    "\n",
    "      encoded_input = tf.one_hot(pred, len(vocabulary))\n",
    "      encoded_input = tf.reshape(encoded_input, [1, 1, len(vocabulary)])\n",
    "\n",
    "    return generated_sequence\n",
    "\n",
    "  def on_epoch_begin(self, epoch, logs):\n",
    "    print(\"Epoch: {}\".format(epoch))\n",
    "    for start_seq in self.start_sequences:\n",
    "      print(\"Starting Sequence: {}\".format(start_seq))\n",
    "      for temp in self.temperature:\n",
    "        print(\"Temperature: {}\".format(temp))\n",
    "        generated_seq = self.generate_text(start_seq, temp)\n",
    "        print(generated_seq)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j1jal7WWK7kM"
   },
   "outputs": [],
   "source": [
    "valid_callback = TextGenerationCallback(inference_model=inference_model,\n",
    "                                        start_sequences=['Un giorno'], generation_length=100, \n",
    "                                        temperature=[0.2, 0.5, 1.0, 1.2])\n",
    "callbacks.append(valid_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ewkV8941pEto"
   },
   "outputs": [],
   "source": [
    "train_model.fit(x=train_dataset,\n",
    "                epochs=100,  #### set repeat in training dataset\n",
    "                steps_per_epoch=int(np.ceil(X_enc.shape[0] / bs)),\n",
    "                callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Text_Generation_Char_Level_.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
